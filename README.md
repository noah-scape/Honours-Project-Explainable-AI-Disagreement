# Honours-Project-Explainable-AI-Disagreement
This project deploys machine learning classifiers on the UNSW NB-15 dataset to analyse the rationale behind model decision making with XAI tools: SHAP and LIME
In this project, four principal machine learning models XGBoost, Decision Trees, Random Forest and Multi-layer Perceptron algorithms are trained on the UNSW NB-15 dataset to detect malicious network activity. These modelsâ€™ results are then analysed by Explainable AI techniques SHAP and LIME which rank the feature importance weights according to predictions. The disagreement between SHAP and LIME explanations are measured by three newly founded metrics, the Narrative Flexibility Score (NFS), the Manipulation Potential Index (MPI) and the composite score of Fairwashing Vulnerability Score (FVS). These scores aim to quantify the inconsistency between SHAP and LIME while assessing the risk of fairwashing to XAI methods. 
